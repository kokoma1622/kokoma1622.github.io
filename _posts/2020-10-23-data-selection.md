---
title: "[ML] Data Selection"
date: 2020-10-23 18:30:33 -0400
tags: [machine learning, data preprocessing]
categories: ML
---

어떤 데이터를 쓸 것인가? 는 패치 버전, 분, 10명의 챔피언, 레벨, cs, kda로 정했다.

다이아 이상의 게임들만 데이터로 이용하기로 했기 때문에, DB에 들어있는 게임 전부를 사용할 순 없었다.
또한 다시하기 게임 역시 걸러야했는데, 이는 game_summary table의 duration 변수를 통해 가능했다.
오픈 게임도 꽤 있었기에, 15분 이상 지속된 게임만 데이터로 사용했다.

평균 티어 다이아 이상이면서 게임 시간이 15분이 넘는 게임만 남기니 패치 별 약 3만개의 게임이 남았다.
정확히는 패치 버전 별 사용 가능한 게임 수가 3만~6만으로 다양했지만,
학습 데이터의 밸런스를 위해 버전 별 같은 게임 수를 사용해야하니 3만개로 고정하였다.

게임의 평균 시간이 약 24 ~ 25분으로, 게임 1개 당 평균 25개의 image가 나오게 된다.
여기서 image란 28x28x12 size의 해당 분 데이터를 가진 입력 데이터이다.
단순 계산으로 모든 데이터를 db에 넣으면, 20 x 30k x 28 x 28 x 12 = 137G 라는 말도안되는 용량이 된다.
그리고 데이터셋 세이브와 로드의 편의성을 위해 패치 버전 별 게임 100개 단위로 묶어 저장하여,
패치 버전 당 250개의 train dataset과 50개의 test dataset이 있다.
어제 글에도 썼지만, 일단 구글드라이브 용량을 확장시킨 뒤 넣어놨다.

용량의 문제와 더불어 경기 극초반의 데이터는 승패 예측에 유의미한 정도를 담기 어려울 수 있다 판단하여,
초반 10분을 제외한 데이터셋을 다시 만들었다.
게임 당 평균 image 개수가 25개이기 때문에, 앞의 10분을 제외하면 image 전체 수가 40% 정도로 줄어들게 된다.
이래도 55G인건 안 비밀.

이후로는 데이터를 랜덤추출하여 사용하기로 했다.
게임의 모든 분 데이터를 사용하는 것이 아니라, 게임에서 5 ~ 10개의 분만 선택하여 입력 데이터셋으로 사용하는 것이다.
앞의 방법으로는 train dataset과 test dataset은 필연적으로 다른 게임을 사용해야 했다면,
이 방법으로는 11개의 분을 선택하여 10개는 train으로 1개는 test로 사용할 수 있다.
random.shuffle을 활용하여 게임의 길이 안에 속하는 11개의 분을 먼저 고르고, train dataset과 test dataset을 만들었다.
일단 버전 별 5000개의 게임만 사용하여 1M 개의 train dataset과 100k 개의 test dataset을 만들었다.
더 많은 게임을 데이터셋에 넣는다면, 각 게임에 train dataset을 위해 10개를 고르던 것을 다소 줄여 용량을 줄일 수 있다.
이 방법을 사용하면 각 게임 당 image 수가 동일하기 때문에,
np.append를 사용하지 않고 dataset을 np로 미리 선언해두고 원하는 위치에 넣을 수 있는 장점이 있다.

dataset을 다 넣은 후, 순서를 randomize하여 저장한다.
이 때 x와 y(입력과 정답)의 순서쌍이 흐트러지면 안 되기 때문에, 단순 np.shuffle을 쓰기보다, 다른 방식을 사용하였다.
RAM 제한이 딱히 없다면 shuffle에 seed를 써도 상관은 없긴 하고,
dataset 길이에 맞게 np.random.permutation으로 index를 randomize하여 재배치하는 것으로 순서를 섞었다.
